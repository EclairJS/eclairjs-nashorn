<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>JSDoc: Source: SparkContext.js</title>

    <script src="scripts/prettify/prettify.js"> </script>
    <script src="scripts/prettify/lang-css.js"> </script>
    <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <link type="text/css" rel="stylesheet" href="styles/prettify-tomorrow.css">
    <link type="text/css" rel="stylesheet" href="styles/jsdoc-default.css">
</head>

<body>

<div id="main">

    <h1 class="page-title">Source: SparkContext.js</h1>

    



    
    <section>
        <article>
            <pre class="prettyprint source linenums"><code>/*
 * Copyright 2015 IBM Corp.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */


var initSparkContext = function (conf) {
    var logger = Logger.getLogger("SparkContext_js");
    if (typeof kernel !== 'undefined') {
        if (kernel.javaSparkContext() != null) {
            return kernel.javaSparkContext();
        } else {
            kernel.createSparkContext(Utils.unwrapObject(conf));
            return kernel.javaSparkContext();
        }
    }

    /*
     * Create a new JavaSparkContext from a conf
     *
     */
    var jvmSC = new org.apache.spark.api.java.JavaSparkContext(Utils.unwrapObject(conf));
    /*
     * add the jar for the cluster
     */
    var decodedPath = org.eclairjs.nashorn.Utils.jarLoc();
    var devEnvPath = "/target/classes/";
    var jarEnvPath = ".jar";
    logger.info("jar decodedPath = " + decodedPath);
    if (decodedPath.indexOf(devEnvPath,
            decodedPath.length - devEnvPath.length) !== -1) {
        /*
         * we are in the dev environment I hope...
         */
        jvmSC.addJar(decodedPath + "../eclairjs-nashorn-0.1.jar");
    } else if (decodedPath.indexOf(jarEnvPath,
            decodedPath.length - jarEnvPath.length) !== -1) {
        /*
         * We are running from a jar
         */
        jvmSC.addJar(decodedPath);
    }

    return jvmSC
};
/**
 *
 * @constructor
 * @classdesc A JavaScript-friendly version of SparkContext that returns RDDs
 * Only one SparkContext may be active per JVM. You must stop() the active SparkContext before creating a new one.
 * This limitation may eventually be removed; see SPARK-2243 for more details.
 * @param {SparkConf} conf - a object specifying Spark parameters
 */
var SparkContext = function () {
    var jvmObj;
    this.logger = Logger.getLogger("SparkContext_js");
    if (arguments.length == 2) {
        var conf = new SparkConf()
        conf.setMaster(arguments[0])
        conf.setAppName(arguments[1])
        jvmObj = initSparkContext(conf)
    } else if (arguments.length==1 &amp;&amp;
        (arguments[0] instanceof org.apache.spark.api.java.JavaSparkContext))
    {
        jvmObj = arguments[0];
    } else {
        jvmObj = initSparkContext(arguments[0])
    }
    JavaWrapper.call(this, jvmObj);
    this.logger.debug(this.version());
};

SparkContext.prototype = Object.create(JavaWrapper.prototype);

//Set the "constructor" property to refer to SparkContext
SparkContext.prototype.constructor = SparkContext;

/**
 * Return a copy of this SparkContext's configuration. The configuration ''cannot'' be
 * changed at runtime.
 * @returns {SparkConf}
 */
SparkContext.prototype.getConf = function () {
    var javaObject = this.getJavaObject().getConf();
    return new SparkConf(javaObject);
};


/**
 * @returns {string[]}
 */
SparkContext.prototype.jars = function () {
    return this.getJavaObject().jars();
};


/**
 * @returns {string[]}
 */
SparkContext.prototype.files = function () {
    return this.getJavaObject().files();
};


/**
 * @returns {string}
 */
SparkContext.prototype.master = function () {
    return this.getJavaObject().master();
};


/**
 * @returns {string}
 */
SparkContext.prototype.appName = function () {
    return this.getJavaObject().appName();
};


/**
 * @returns {boolean}
 */
SparkContext.prototype.isLocal = function () {
    return this.getJavaObject().isLocal();
};

/**
 * @returns {boolean}  true if context is stopped or in the midst of stopping.
 */
SparkContext.prototype.isStopped = function () {
    return this.getJavaObject().isStopped();
};


/**
 * @returns {SparkStatusTracker}
 */
SparkContext.prototype.statusTracker = function () {
    var javaObject = this.getJavaObject().statusTracker();
    return new SparkStatusTracker(javaObject);
};

/**
 * A unique identifier for the Spark application.
 * Its format depends on the scheduler implementation.
 * (i.e.
 *  in case of local spark app something like 'local-1433865536131'
 *  in case of YARN something like 'application_1433865536131_34483'
 * )
 * @returns {string}
 */
SparkContext.prototype.applicationId = function () {
    return this.getJavaObject().applicationId();
};


/**
 * @returns {string}
 */
SparkContext.prototype.applicationAttemptId = function () {
    return this.getJavaObject().applicationAttemptId();
};


/**
 * @param {string} logLevel  The desired log level as a string.
 * Valid log levels include: ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN
 */
SparkContext.prototype.setLogLevel = function (logLevel) {
    this.getJavaObject().setLogLevel(logLevel);
};


/**
 * initLocalProperties
 */
SparkContext.prototype.initLocalProperties = function () {
    this.getJavaObject().initLocalProperties();
};


/**
 * Set a local property that affects jobs submitted from this thread, such as the
 * Spark fair scheduler pool.
 * @param {string}
 * @param {string}
 */
SparkContext.prototype.setLocalProperty = function (key, value) {
    this.getJavaObject().setLocalProperty(key, value);
};


/**
 * Get a local property set in this thread, or null if it is missing. See
 * {@link setLocalProperty}.
 * @param {string}
 * @returns {string}
 */
SparkContext.prototype.getLocalProperty = function (key) {
    return this.getJavaObject().getLocalProperty(key);
};


/**
 * @param {string}
 */
SparkContext.prototype.setJobDescription = function (value) {
    this.getJavaObject().setJobDescription(value);
};


/**
 * Assigns a group ID to all the jobs started by this thread until the group ID is set to a
 * different value or cleared.
 *
 * Often, a unit of execution in an application consists of multiple Spark actions or jobs.
 * Application programmers can use this method to group all those jobs together and give a
 * group description. Once set, the Spark web UI will associate such jobs with this group.
 *
 * The application can also use {@link cancelJobGroup} to cancel all
 * running jobs in this group. For example,
 * @param {string}
 * @param {string}
 * @param {boolean}
 */
SparkContext.prototype.setJobGroup = function (groupId, description, interruptOnCancel) {
    this.getJavaObject().setJobGroup(groupId, description, interruptOnCancel);
};


/**
 * clearJobGroup
 */
SparkContext.prototype.clearJobGroup = function () {
    this.getJavaObject().clearJobGroup();
};


/**
 * Create an {@link Accumulable} shared variable of the given type, to which tasks can "add" values with add.
 * Only the master can access the accumuable's value.
 *
 * @param {object} initialValue
 * @param {AccumulableParam} param
 * @param {string} name of  the accumulator for display in Spark's web UI.
 * @returns {Accumulable}
 */
SparkContext.prototype.accumulable = function (initialValue, param, name) {
    return new Accumulable(initialValue, param, name);

};
/**
 * Create an {@link Accumulator}  variable, which tasks can "add" values to using the add method.
 * Only the master can access the accumulator's value.
 *
 * @param {int | float} initialValue
 * @param {string | AccumulableParam} [name] of  the accumulator for display in Spark's web UI. or param.  defaults to FloatAccumulatorParam
 * @param {AccumulableParam} [param]  defaults to FloatAccumulatorParam, use only if also specifying name
 * @returns {Accumulator}
 */
SparkContext.prototype.accumulator = function () {
    var initialValue = arguments[0];
    var name;
    var param = new FloatAccumulatorParam();
    this.logger.debug("accumulator " + initialValue);

    if (arguments[1]) {
        if (typeof arguments[1] === "string") {
            name = arguments[1];
            if (arguments[2]) {
                param = arguments[2];
            }
        } else {
            param = arguments[1];
        }
    }
    return new Accumulator(initialValue, param, name);

};
/**
 * Create an Accumulator integer variable, which tasks can "add" values to using the add method.
 * Only the master can access the accumulator's value.
 * @param {int} initialValue
 * @param {string} name of  the accumulator for display in Spark's web UI.
 * @returns {Accumulator}
 */
SparkContext.prototype.intAccumulator = function (initialValue, name) {
    return new Accumulator(initialValue, new IntAccumulatorParam(), name);

};
/**
 * Create an Accumulator float variable, which tasks can "add" values to using the add method.
 * Only the master can access the accumulator's value.
 * @param {float} initialValue
 * @param {string} name of  the accumulator for display in Spark's web UI.
 * @returns {Accumulator}
 */
SparkContext.prototype.floatAccumulator = function (initialValue, name) {
    return new Accumulator(initialValue, new FloatAccumulatorParam(), name);

};
/**
 *  Add a file to be downloaded with this Spark job on every node. The path passed can be either a local file,
 * a file in HDFS (or other Hadoop-supported filesystems), or an HTTP, HTTPS or FTP URI.
 * To access the file in Spark jobs, use SparkFiles.get(fileName) to find its download location.
 * @param {string} path - Path to the file
 */
SparkContext.prototype.addFile = function (path) {
    this.getJavaObject().addFile(path);
};
/**
 * Adds a JAR dependency for all tasks to be executed on this SparkContext in the future. The path passed can be either a local file, a file in HDFS (or other Hadoop-supported filesystems), or an HTTP, HTTPS or FTP URI.
 * @param {string} path - Path to the jar
 */
SparkContext.prototype.addJar = function (path) {
    //public void addJar(java.lang.String path)
    this.getJavaObject().addJar(path);
};
/**
 * Broadcast a read-only variable to the cluster, returning a Broadcast object for reading it in distributed functions.
 * The variable will be sent to each cluster only once.
 * @param {object} value
 * @returns {Broadcast}
 */
SparkContext.prototype.broadcast = function (value) {
    return this.getJavaObject().broadcast(value);
};

/**
 * Distribute a local Scala collection to form an RDD.
 * @param {array} list
 * @param {integer} [numSlices]
 * @returns {RDD}
 */
SparkContext.prototype.parallelize = function (list, numSlices) {
    //public &lt;T> JavaRDD&lt;T> parallelize(java.util.List&lt;T> list, int numSlices)
    var list_uw = [];
    list.forEach(function (item) {
        list_uw.push(Utils.unwrapObject(item));
        //list_uw.push(Serialize.jsToJava(item));
    });
    if (numSlices) {
        return new RDD(this.getJavaObject().parallelize(list_uw, numSlices));
    } else {
        return new RDD(this.getJavaObject().parallelize(list_uw));
    }

};


/**
 * Distribute a local collection to form an RDD.
 * @param {array} list array of Tuple 2
 * @param {integer} numSlices
 * @returns {PairRDD}
 */
SparkContext.prototype.parallelizePairs = function (list, numSlices) {
    //public &lt;T> JavaRDD&lt;T> parallelize(java.util.List&lt;T> list, int numSlices)
    var list_uw = [];
    list.forEach(function (item) {
        list_uw.push(Utils.unwrapObject(item));
    });
    if (numSlices) {
        return new PairRDD(this.getJavaObject().parallelizePairs(list_uw, numSlices));
    } else {
        return new PairRDD(this.getJavaObject().parallelizePairs(list_uw));
    }

};
/**
 * Creates a new RDD[Long] containing elements from `start` to `end`(exclusive), increased by
 * `step` every element.
 *
 * @note if we need to cache this RDD, we should make sure each partition does not exceed limit.
 *
 * @param {number} start  the start value.
 * @param {number} end  the end value.
 * @param {number} step  the incremental step
 * @param {number} numSlices  the partition number of the new RDD.
 * @returns {RDD}
 */
SparkContext.prototype.range = function (start, end, step, numSlices) {
    var javaObject = this.getJavaObject().range(start, end, step, numSlices);
    return new RDD(javaObject);
};


/**
 * Read a text file from HDFS, a local file system (available on all nodes), or any Hadoop-supported file system URI,
 * and return it as an RDD of Strings.
 * @param {string} path - path to file
 * @param {int} [minPartitions]
 * @returns {RDD}
 */
SparkContext.prototype.textFile = function (path, minPartitions) {
    if (minPartitions) {
        return new RDD(this.getJavaObject().textFile(path, minPartitions));
    } else {
        return new RDD(this.getJavaObject().textFile(path));
    }

};


/**
 * Read a directory of text files from HDFS, a local file system (available on all nodes), or any
 * Hadoop-supported file system URI. Each file is read as a single record and returned in a
 * key-value pair, where the key is the path of each file, the value is the content of each file.
 *
 * &lt;p> For example, if you have the following files:
 * @example
 *   hdfs://a-hdfs-path/part-00000
 *   hdfs://a-hdfs-path/part-00001
 *   ...
 *   hdfs://a-hdfs-path/part-nnnnn
 *
 *
 * Do `var rdd = sparkContext.wholeTextFile("hdfs://a-hdfs-path")`,
 *
 * &lt;p> then `rdd` contains
 * @example
 *   (a-hdfs-path/part-00000, its content)
 *   (a-hdfs-path/part-00001, its content)
 *   ...
 *   (a-hdfs-path/part-nnnnn, its content)
 *
 *
 * @note Small files are preferred, large file is also allowable, but may cause bad performance.
 * @note On some filesystems, `.../path/&amp;#42;` can be a more efficient way to read all files
 *       in a directory rather than `.../path/` or `.../path`
 *
 * @param {string} path  Directory to the input data files, the path can be comma separated paths as the
 *             list of inputs.
 * @param {number} minPartitions  A suggestion value of the minimal splitting number for input data.
 * @returns {RDD}
 */
SparkContext.prototype.wholeTextFiles = function (path, minPartitions) {
    var javaObject = this.getJavaObject().wholeTextFiles(path, minPartitions);
    return new RDD(javaObject);
};

/**
 * Set the directory under which RDDs are going to be checkpointed.
 * The directory must be a HDFS path if running on a cluster.
 * @param {string} dir
 */
SparkContext.prototype.setCheckpointDir = function (dir) {
    this.getJavaObject().setCheckpointDir(dir);
};

/**
 * Shut down the SparkContext.
 */
SparkContext.prototype.stop = function () {
    this.getJavaObject().stop();
};

/**
 * The version of EclairJS and Spark on which this application is running.
 * @returns {string}
 */
SparkContext.prototype.version = function () {
    var javaVersion = java.lang.System.getProperty("java.version");
    var jv = javaVersion.split(".");
    var wrongJavaVersionString = "Java 1.8.0_60 or greater required for EclairJS";
    var wrongSparkVersionString = "Spark 1.5.1 or greater required for EclairJS";
    if (jv[0] &lt; 2) {
        if (jv[0] == 1) {
            if (jv[1] &lt; 8) {
                throw wrongJavaVersionString;
            } else {
                if (jv[1] == 8) {
                    // we are at 1.8
                    var f = jv[2]
                    var fix = f.split("_");
                    if ((fix[0] &lt; 1) &amp;&amp; (fix[1] &lt; 60)) {
                        // less than 1.8.0_60
                        throw wrongJavaVersionString;
                    }
                } else {
                    // 1.9 or greater
                }
            }
        } else {
            throw wrongJavaVersionString;
        }

    } else {
        // versions is 2.0 or greater
    }
    var sparkVersion = this.getJavaObject().version();
    var sv = sparkVersion.split(".");
    if (sv[0] &lt; 2) {
        if (sv[0] == 1) {
            if (sv[1] &lt; 5) {
                throw wrongSparkVersionString;
            } else {
                if (sv[1] == 5) {
                    // we are at 1.5
                    if (sv[2] &lt; 1) {
                        // less than 1.5.1
                        throw wrongSparkVersionString;
                    }
                } else {
                    // 1.5 or greater
                }
            }
        } else {
            throw wrongSparkVersionString;
        }

    } else {
        // versions is 2.0 or greater
    }
    return "EclairJS-nashorn 0.1 Spark " + sparkVersion;
};

/**
 * Zip up a file in a directory to preserve it's path and add it to worker node for download via addFile.
 */
SparkContext.prototype.addModule = function(module) {
    //print("SparkContext.addModule: "+module.toString());
    //var folder = module.modname.slice(0, module.modname.lastIndexOf("\/")),
    var parent = ModuleUtils.getParent(module);
    //print("SparkContext.addModule parent: "+parent);
 
    var folder = (parent &amp;&amp; parent.subdir &amp;&amp; module.subdir.indexOf(parent.subdir) > -1) ? parent.subdir : (module.subdir || "."),
        zipfile = parent &amp;&amp; parent.zipfile ? parent.zipfile.replace(".zip", "_child_"+Date.now()+".zip")  : "module_"+Date.now()+".zip",
        filename = module.id.slice(module.id.lastIndexOf("\/")+1, module.id.length);
    //print("SparkContext.addModule folder: "+folder);
    //print("SparkContext.addModule filename: "+filename);
    try {
        org.eclairjs.nashorn.Utils.zipFile(folder, zipfile, [filename]);
        //print("SparkContext.addModule zipped file now going to try and addFile: "+zipfile);
        this.getJavaObject().addFile(zipfile);
        module.zipfile = zipfile;
    } catch (exc) {
        print("Cannot add module: "+module.modname);
        print(exc);
    }
};

/**
 * Zip up all required files not in JAR to preserve paths and add it to worker node for download via addFile.
 */
SparkContext.prototype.addCustomModules = function() {
    var mods = ModuleUtils.getModulesByType({type:"core", value:false});
    //print("addingCustomMods: "+mods.toString());
    var folder = ".", zipfile = ModuleUtils.defaultZipFile, filenames = [];
    mods.forEach(function(mod){
        filenames.push(mod.id.slice(mod.id.lastIndexOf("\/")+1, mod.id.length));
    });
    //print("SparkContext.addModule folder: "+folder);
    //print("SparkContext.addModule filenames: "+filenames.toString());
    try {
        org.eclairjs.nashorn.Utils.zipFile(folder, zipfile, filenames);
        this.getJavaObject().addFile(zipfile);
    } catch (exc) {
        print("Cannot add non core modules: "+filenames.toString());
        print(exc);
    }
};
</code></pre>
        </article>
    </section>




</div>

<nav>
    <h2><a href="index.html">Home</a></h2><h3>Modules</h3><ul><li><a href="module-eclairjs_ml_feature.html">eclairjs/ml/feature</a></li><li><a href="module-eclairjs_mllib.html">eclairjs/mllib</a></li><li><a href="module-eclairjs_mllib_classification.html">eclairjs/mllib/classification</a></li><li><a href="module-eclairjs_mllib_clustering.html">eclairjs/mllib/clustering</a></li><li><a href="module-eclairjs_mllib_evaluation.html">eclairjs/mllib/evaluation</a></li><li><a href="module-eclairjs_mllib_feature.html">eclairjs/mllib/feature</a></li><li><a href="module-eclairjs_mllib_fpm.html">eclairjs/mllib/fpm</a></li><li><a href="module-eclairjs_mllib_linalg.html">eclairjs/mllib/linalg</a></li><li><a href="module-eclairjs_mllib_linalg_distributed.html">eclairjs/mllib/linalg/distributed</a></li><li><a href="module-eclairjs_mllib_optimization.html">eclairjs/mllib/optimization</a></li><li><a href="module-eclairjs_mllib_random.html">eclairjs/mllib/random</a></li><li><a href="module-eclairjs_mllib_recommendation.html">eclairjs/mllib/recommendation</a></li><li><a href="module-eclairjs_mllib_regression.html">eclairjs/mllib/regression</a></li><li><a href="module-eclairjs_mllib_tree.html">eclairjs/mllib/tree</a></li><li><a href="module-eclairjs_mllib_tree_configuration.html">eclairjs/mllib/tree/configuration</a></li><li><a href="module-eclairjs_mllib_tree_loss.html">eclairjs/mllib/tree/loss</a></li><li><a href="module-eclairjs_mllib_tree_model.html">eclairjs/mllib/tree/model</a></li><li><a href="module-eclairjs_sql.html">eclairjs/sql</a></li><li><a href="module-eclairjs_sql_types.html">eclairjs/sql/types</a></li><li><a href="module-eclairjs_storage.html">eclairjs/storage</a></li><li><a href="module-eclairjs_streaming.html">eclairjs/streaming</a></li><li><a href="module-eclairjs_streaming_dstream.html">eclairjs/streaming/dstream</a></li><li><a href="module-eclairjs_streaming_kafka.html">eclairjs/streaming/kafka</a></li><li><a href="module-eclairjs_streaming_twitter.html">eclairjs/streaming/twitter</a></li></ul><h3>Classes</h3><ul><li><a href="Accumulable.html">Accumulable</a></li><li><a href="AccumulableParam.html">AccumulableParam</a></li><li><a href="Accumulator.html">Accumulator</a></li><li><a href="FloatAccumulatorParam.html">FloatAccumulatorParam</a></li><li><a href="FloatRDD.html">FloatRDD</a></li><li><a href="FutureAction.html">FutureAction</a></li><li><a href="HashPartitioner.html">HashPartitioner</a></li><li><a href="IntAccumulatorParam.html">IntAccumulatorParam</a></li><li><a href="List.html">List</a></li><li><a href="Logger.html">Logger</a></li><li><a href="module-eclairjs_ml_feature.Word2Vec.html">Word2Vec</a></li><li><a href="module-eclairjs_ml_feature.Word2VecModel.html">Word2VecModel</a></li><li><a href="module-eclairjs_mllib.MLUtils.html">MLUtils</a></li><li><a href="module-eclairjs_mllib_classification.ClassificationModel.html">ClassificationModel</a></li><li><a href="module-eclairjs_mllib_classification.LogisticRegressionModel.html">LogisticRegressionModel</a></li><li><a href="module-eclairjs_mllib_classification.LogisticRegressionWithLBFGS.html">LogisticRegressionWithLBFGS</a></li><li><a href="module-eclairjs_mllib_classification.LogisticRegressionWithSGD.html">LogisticRegressionWithSGD</a></li><li><a href="module-eclairjs_mllib_classification.NaiveBayes.html">NaiveBayes</a></li><li><a href="module-eclairjs_mllib_classification.NaiveBayesModel.html">NaiveBayesModel</a></li><li><a href="module-eclairjs_mllib_classification.SVMModel.html">SVMModel</a></li><li><a href="module-eclairjs_mllib_classification.SVMWithSGD.html">SVMWithSGD</a></li><li><a href="module-eclairjs_mllib_clustering.BisectingKMeans.html">BisectingKMeans</a></li><li><a href="module-eclairjs_mllib_clustering.BisectingKMeansModel.html">BisectingKMeansModel</a></li><li><a href="module-eclairjs_mllib_clustering.DistributedLDAModel.html">DistributedLDAModel</a></li><li><a href="module-eclairjs_mllib_clustering.KMeans.html">KMeans</a></li><li><a href="module-eclairjs_mllib_clustering.KMeansModel.html">KMeansModel</a></li><li><a href="module-eclairjs_mllib_clustering.LDA.html">LDA</a></li><li><a href="module-eclairjs_mllib_clustering.LDAModel.html">LDAModel</a></li><li><a href="module-eclairjs_mllib_clustering.LocalLDAModel.html">LocalLDAModel</a></li><li><a href="module-eclairjs_mllib_clustering.PowerIterationClustering.html">PowerIterationClustering</a></li><li><a href="module-eclairjs_mllib_clustering.PowerIterationClusteringAssignment.html">PowerIterationClusteringAssignment</a></li><li><a href="module-eclairjs_mllib_clustering.PowerIterationClusteringModel.html">PowerIterationClusteringModel</a></li><li><a href="module-eclairjs_mllib_evaluation.BinaryClassificationMetrics.html">BinaryClassificationMetrics</a></li><li><a href="module-eclairjs_mllib_evaluation.MulticlassMetrics.html">MulticlassMetrics</a></li><li><a href="module-eclairjs_mllib_evaluation.MultilabelMetrics.html">MultilabelMetrics</a></li><li><a href="module-eclairjs_mllib_evaluation.RankingMetrics.html">RankingMetrics</a></li><li><a href="module-eclairjs_mllib_evaluation.RegressionMetrics.html">RegressionMetrics</a></li><li><a href="module-eclairjs_mllib_feature.Word2Vec.html">Word2Vec</a></li><li><a href="module-eclairjs_mllib_feature.Word2VecModel.html">Word2VecModel</a></li><li><a href="module-eclairjs_mllib_fpm.AssociationRules.html">AssociationRules</a></li><li><a href="module-eclairjs_mllib_fpm.FPGrowth.html">FPGrowth</a></li><li><a href="module-eclairjs_mllib_fpm.FPGrowthModel.html">FPGrowthModel</a></li><li><a href="module-eclairjs_mllib_fpm.FreqItemset.html">FreqItemset</a></li><li><a href="module-eclairjs_mllib_fpm.PrefixSpan.html">PrefixSpan</a></li><li><a href="module-eclairjs_mllib_fpm.PrefixSpanFreqSequence.html">PrefixSpanFreqSequence</a></li><li><a href="module-eclairjs_mllib_fpm.PrefixSpanModel.html">PrefixSpanModel</a></li><li><a href="module-eclairjs_mllib_fpm.Rule.html">Rule</a></li><li><a href="module-eclairjs_mllib_linalg.DenseMatrix.html">DenseMatrix</a></li><li><a href="module-eclairjs_mllib_linalg.DenseVector.html">DenseVector</a></li><li><a href="module-eclairjs_mllib_linalg.Matrices.html">Matrices</a></li><li><a href="module-eclairjs_mllib_linalg.Matrix.html">Matrix</a></li><li><a href="module-eclairjs_mllib_linalg.QRDecomposition.html">QRDecomposition</a></li><li><a href="module-eclairjs_mllib_linalg.SingularValueDecomposition.html">SingularValueDecomposition</a></li><li><a href="module-eclairjs_mllib_linalg.SparseMatrix.html">SparseMatrix</a></li><li><a href="module-eclairjs_mllib_linalg.SparseVector.html">SparseVector</a></li><li><a href="module-eclairjs_mllib_linalg.Vector.html">Vector</a></li><li><a href="module-eclairjs_mllib_linalg.Vectors.html">Vectors</a></li><li><a href="module-eclairjs_mllib_linalg.VectorUDT.html">VectorUDT</a></li><li><a href="module-eclairjs_mllib_linalg_distributed.DistributedMatrix.html">DistributedMatrix</a></li><li><a href="module-eclairjs_mllib_linalg_distributed.RowMatrix.html">RowMatrix</a></li><li><a href="module-eclairjs_mllib_optimization.Gradient.html">Gradient</a></li><li><a href="module-eclairjs_mllib_optimization.LBFGS.html">LBFGS</a></li><li><a href="module-eclairjs_mllib_optimization.LogisticGradient.html">LogisticGradient</a></li><li><a href="module-eclairjs_mllib_optimization.SquaredL2Updater.html">SquaredL2Updater</a></li><li><a href="module-eclairjs_mllib_optimization.Updater.html">Updater</a></li><li><a href="module-eclairjs_mllib_random.RandomRDDs.html">RandomRDDs</a></li><li><a href="module-eclairjs_mllib_recommendation.ALS.html">ALS</a></li><li><a href="module-eclairjs_mllib_recommendation.MatrixFactorizationModel.html">MatrixFactorizationModel</a></li><li><a href="module-eclairjs_mllib_recommendation.Rating.html">Rating</a></li><li><a href="module-eclairjs_mllib_regression.GeneralizedLinearModel.html">GeneralizedLinearModel</a></li><li><a href="module-eclairjs_mllib_regression.LabeledPoint.html">LabeledPoint</a></li><li><a href="module-eclairjs_mllib_regression.LinearRegressionModel.html">LinearRegressionModel</a></li><li><a href="module-eclairjs_mllib_regression.LinearRegressionWithSGD.html">LinearRegressionWithSGD</a></li><li><a href="module-eclairjs_mllib_tree.DecisionTree.html">DecisionTree</a></li><li><a href="module-eclairjs_mllib_tree.GradientBoostedTrees.html">GradientBoostedTrees</a></li><li><a href="module-eclairjs_mllib_tree.RandomForest.html">RandomForest</a></li><li><a href="module-eclairjs_mllib_tree_configuration.BoostingStrategy.html">BoostingStrategy</a></li><li><a href="module-eclairjs_mllib_tree_configuration.Strategy.html">Strategy</a></li><li><a href="module-eclairjs_mllib_tree_loss.Loss.html">Loss</a></li><li><a href="module-eclairjs_mllib_tree_model.DecisionTreeModel.html">DecisionTreeModel</a></li><li><a href="module-eclairjs_mllib_tree_model.GradientBoostedTreesModel.html">GradientBoostedTreesModel</a></li><li><a href="module-eclairjs_mllib_tree_model.RandomForestModel.html">RandomForestModel</a></li><li><a href="module-eclairjs_sql.Column.html">Column</a></li><li><a href="module-eclairjs_sql.DataFrame.html">DataFrame</a></li><li><a href="module-eclairjs_sql.DataFrameHolder.html">DataFrameHolder</a></li><li><a href="module-eclairjs_sql.DataFrameNaFunctions.html">DataFrameNaFunctions</a></li><li><a href="module-eclairjs_sql.DataFrameReader.html">DataFrameReader</a></li><li><a href="module-eclairjs_sql.DataFrameStatFunctions.html">DataFrameStatFunctions</a></li><li><a href="module-eclairjs_sql.DataFrameWriter.html">DataFrameWriter</a></li><li><a href="module-eclairjs_sql.functions.html">functions</a></li><li><a href="module-eclairjs_sql.GroupedData.html">GroupedData</a></li><li><a href="module-eclairjs_sql.Row.html">Row</a></li><li><a href="module-eclairjs_sql.RowFactory.html">RowFactory</a></li><li><a href="module-eclairjs_sql.SQLContext.html">SQLContext</a></li><li><a href="module-eclairjs_sql.SQLContext.QueryExecution.html">QueryExecution</a></li><li><a href="module-eclairjs_sql.SQLContext.SparkPlanner.html">SparkPlanner</a></li><li><a href="module-eclairjs_sql.SQLContext.SQLSession.html">SQLSession</a></li><li><a href="module-eclairjs_sql.SqlDate.html">SqlDate</a></li><li><a href="module-eclairjs_sql.SqlTimestamp.html">SqlTimestamp</a></li><li><a href="module-eclairjs_sql_types.ArrayType.html">ArrayType</a></li><li><a href="module-eclairjs_sql_types.BinaryType.html">BinaryType</a></li><li><a href="module-eclairjs_sql_types.BooleanType.html">BooleanType</a></li><li><a href="module-eclairjs_sql_types.CalendarIntervalType.html">CalendarIntervalType</a></li><li><a href="module-eclairjs_sql_types.DataType.html">DataType</a></li><li><a href="module-eclairjs_sql_types.DataTypes.html">DataTypes</a></li><li><a href="module-eclairjs_sql_types.DateType.html">DateType</a></li><li><a href="module-eclairjs_sql_types.DoubleType.html">DoubleType</a></li><li><a href="module-eclairjs_sql_types.FloatType.html">FloatType</a></li><li><a href="module-eclairjs_sql_types.IntegerType.html">IntegerType</a></li><li><a href="module-eclairjs_sql_types.MapType.html">MapType</a></li><li><a href="module-eclairjs_sql_types.Metadata.html">Metadata</a></li><li><a href="module-eclairjs_sql_types.NullType.html">NullType</a></li><li><a href="module-eclairjs_sql_types.NumericType.html">NumericType</a></li><li><a href="module-eclairjs_sql_types.StringType.html">StringType</a></li><li><a href="module-eclairjs_sql_types.StructField.html">StructField</a></li><li><a href="module-eclairjs_sql_types.StructType.html">StructType</a></li><li><a href="module-eclairjs_sql_types.TimestampType.html">TimestampType</a></li><li><a href="module-eclairjs_storage.StorageLevel.html">StorageLevel</a></li><li><a href="module-eclairjs_streaming.Duration.html">Duration</a></li><li><a href="module-eclairjs_streaming.StreamingContext.html">StreamingContext</a></li><li><a href="module-eclairjs_streaming.Time.html">Time</a></li><li><a href="module-eclairjs_streaming_dstream.DStream.html">DStream</a></li><li><a href="module-eclairjs_streaming_dstream.PairDStream.html">PairDStream</a></li><li><a href="module-eclairjs_streaming_kafka.KafkaUtils.html">KafkaUtils</a></li><li><a href="module-eclairjs_streaming_twitter.TwitterAuthorization.html">TwitterAuthorization</a></li><li><a href="module-eclairjs_streaming_twitter.TwitterUtils.html">TwitterUtils</a></li><li><a href="PairRDD.html">PairRDD</a></li><li><a href="PartialResult.html">PartialResult</a></li><li><a href="Partitioner.html">Partitioner</a></li><li><a href="RangePartitioner.html">RangePartitioner</a></li><li><a href="RDD.html">RDD</a></li><li><a href="SparkConf.html">SparkConf</a></li><li><a href="SparkContext.html">SparkContext</a></li><li><a href="SparkFiles.html">SparkFiles</a></li><li><a href="SparkStatusTracker.html">SparkStatusTracker</a></li><li><a href="Tuple.html">Tuple</a></li></ul><h3>Global</h3><ul><li><a href="global.html#module">module</a></li></ul>
</nav>

<br class="clear">

<footer>
    Documentation generated by <a href="https://github.com/jsdoc3/jsdoc">JSDoc 3.3.2</a> on Wed Apr 13 2016 16:54:51 GMT-0400 (EDT)
</footer>

<script> prettyPrint(); </script>
<script src="scripts/linenumber.js"> </script>
</body>
</html>
